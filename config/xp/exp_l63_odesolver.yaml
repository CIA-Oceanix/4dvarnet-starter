# @package _global_

xp_name: exp_ode_solver_l63


datamodule:
  _target_: src.data_l63.BaseDataModule
  param_datamodule:
    dl_kw: {batch_size: 128, num_workers: 1}
  input_data:
    _target_: src.data_l63.create_l63_ode_solver_datasets #src.data_l63.create_l63_forecast_datasets
    param_dataset:
      NbTraining: 10000 # number of training sequences
      NbTest: 1000 # number of test sequences
      time_step: 1 # subsampling step for the time series
      integration_step: ?? # integration time step for the learnt solver
      dT: ?? # number of integration time steps for the training data 
      dT_test: 64 # number of integration time steps for the test data 
      dt_forecast: ${subtract:${datamodule.input_data.param_dataset.dT},1} # number of simulation time steps, here dT-1, just using the initial condition
      
model:
  _target_: src.models_l63.Lit4dVarNet_L63_OdeSolver #src.models_l63.Lit4dVarNet_L63
  params:
    k_n_grad: ?? # number of 4dvarnet blocks of n_grad grdient-based iterations
    n_grad: 5 # number of gradient-based iterations in each block
    lr: 1e-3  # learning rate (initial value, cosine annealing learning rate)
    batch_size: 128 # batch size
    DimAE: 10 # ?? # parameter for prior Phi, parameterization of the number of channels
    dim_grad_solver: ?? #10 #100 #25 # dimenbsion of the hidden states of the LSTM cell
    dropout: 0. # dropout value in the LSTM solver
    alpha_prior: 0. #1e2 # weighing factor of term ||x-phi(x)||**2 in the training loss
    alpha_mse: 1.e2 # weighing factor of term ||x_true-x_pred||**2 in the training loss
    alpha_mse_implicit: 0. #1e2 # weighing factor of term ||x_true-x_pred||**2 in the training loss
    alpha_gmse: 0. #50. # weighing factor of term (time derivative) ||D_t x_true-D_t x_pred||**2 in the training loss
    lr_grad: 0. #1e-2 #1.e3 # weighing factor of the firxed-step gradient descent term in 4dvarnet optimisation, \beta in equation (10)
    init_state: ?? #'zeros' #'last_state' #'ode_solver' # selected initialization for the simulation window
    suffix_exp: ?? #'-lstm+grad' #
    shapeData_modGrad: #${model.params.shapeData} # shape of the gradient tensor (similar as shapeData for convLSTM solver, one-dimensional for a dense LSTM solver)
      - ${mul:${datamodule.input_data.param_dataset.dT},3} 
    base_ode_solver: 'euler' # 'rk4' # # ode solver used as baseline (reference and possible initialization)
    simu_test_all_steps: True # During the training step, run the simulation for dT (False) or dT_test time steps
  Phi: # selected parameterization for operator phi
    _target_: src.models_l63.Phi_ode # src.models_l63.Phi_unet_like_bilin #    src.models_l63.Phi_unet_like_bilin # ssrc.models_l63.Phi_unet_1_layer_bis #src.models_l63.UNet_3_layers #
    #shapeData: ${model.params.shapeData}
    #DimAE: ${model.params.DimAE}
    name: 'unet-bilin' # 'odenet' # 
#    rateDropout: 0.2
  mod_Grad: # selected parameterization for the gradient-based solver
    _target_: src.solver_l63.model_Grad_with_lstm
    ShapeData: ${model.params.shapeData_modGrad}
    DimLSTM: ${model.params.dim_grad_solver} 
    rateDropout: ${model.params.dropout} 
    sig_lstm_init: ${model.params.sig_lstm_init}
    name: 'lstm-grad'

trainer:
  _target_: pytorch_lightning.Trainer
  inference_mode: False
  #gradient_clip_val: 0.5
  accelerator: gpu
  devices: 1
  precision: 32
  logger: 
    _target_: pytorch_lightning.loggers.CSVLogger
    save_dir: ${hydra:runtime.output_dir}
    name: ${hydra:runtime.choices.xp}
    version: ''
  max_epochs: 400
  callbacks:
    - _target_: src.versioning_cb.VersioningCallback
    - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val_mse
      save_top_k: 3
      filename: 
        _target_: src.models_l63.create_filename_ckpt_odesolver #create_filename_ckpt
        suffix: '-{val_loss:.4f}-{epoch:03d}'#'{val_mse:.4f}-{epoch:03d}'
        params_data: ${datamodule.input_data.param_dataset}
        params_model: ${model.params}
        name_solver: ${model.mod_Grad.name}
        name_phi: ${model.Phi.name}
        
entrypoints:
  - _target_: pytorch_lightning.seed_everything
    seed: 333
#  - _target_: src.train.base_training  # src.train.fine_tuning # 
#    trainer: ${trainer}
#    lit_mod: ${model}
#    dm: ${datamodule}
  - _target_: src.test.base_testing_ode_solver #src.test.base_testing_forecast #
    trainer: ${trainer}
    lit_mod: ${model}
    dm: ${datamodule}
    num_members: 0
    ckpt: 'outputs/2023-07-25/11-05-41/exp_l63_odesolver/checkpointsmodel-odesolver-l63-mse-only-new-dT32_01_31_02-JamesData-init_euler-unet-bilin_10-fclstm-grad05_01_100-val_loss=0.0148-epoch=376.ckpt'
#    ckpt: 'outputs/2023-07-24/15-05-35/base_l63_odesolver/checkpoints/model-odesolver-l63-new-dT04_01_03_02-JamesData-odenet_10-fclstm-grad05_01_20-val_loss=0.1047-epoch=344.ckpt'
#    ckpt: 'outputs/2023-07-21/17-10-09/base_l63_odesolver/checkpoints/model-odesolver-l63-new-dT04_01_03_02-JamesData-unet-bilin_10-fclstm-grad05_02_20-val_loss=0.1530-epoch=380.ckpt'
#    ckpt: 'outputs/2023-07-19/23-56-13/base_l63_forecast/checkpoints/model-l63-l63data-forecast-dt3-dense-dT04-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad10-val_loss=0.1779-epoch=382.ckpt'
#    ckpt: 'outputs/2023-07-19/11-43-27/base_l63_forecast/checkpoints/model-l63-l63data-forecast-unet-dt3-dT04-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad10-val_loss=0.1043-epoch=377.ckpt'
#    ckpt: 'outputs/2023-07-18/23-06-57/base_l63_forecast/checkpoints/model-l63-l63-step2-forecast-dt3-dT04-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad10-val_loss=0.2576-epoch=376.ckpt'
#    ckpt: 'outputs/2023-07-18/08-30-51/base_l63_forecast/checkpoints/model-l63-l63-forecast01-ode-dT04-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad100-val_loss=40.1054-epoch=398.ckpt'
#    ckpt: 'outputs/2023-07-18/00-28-36/base_l63_forecast/checkpoints/model-l63-forecast-lstm-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_08-dgrad100-val_loss=0.0464-epoch=258.ckpt'
#    ckpt: 'outputs/2023-07-18/00-08-31//base_l63_forecast/checkpoints/model-l63-new-l63data-forecast-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad100-val_loss=0.0190-epoch=019.ckpt'
#    ckpt: 'outputs/2023-07-17/23-34-12/base_l63_forecast/checkpoints/model-l63-new-l63data-forecast-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad100-val_loss=0.0036-epoch=043.ckpt'


defaults:
  - /xp/base_l63_odesolver
  