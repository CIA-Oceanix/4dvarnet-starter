# @package _global_

xp_name: baseline


datamodule:
  _target_: src.data_l63.BaseDataModule
  param_datamodule:
    dl_kw: {batch_size: 128, num_workers: 1}
  input_data:
    _target_: src.data_l63.create_l63_ode_solver_datasets #src.data_l63.create_l63_forecast_datasets
    param_dataset:
#      path_l63_dataset: '../../Dataset4DVarNet/dataset_l63_normalized_20230717.nc' #'../../Dataset4DVarNet/dataset_L63_with_noise.nc'# '../../Dataset4DVarNet/dataset_L63_with_noise.nc'
#      genSuffixObs: 'JamesData'
#      flag_load_all_data: False # keep unchanged
#      flag_generate_L63_data: False #keep unchanged, False: load a pre-generated L63 dataset
      NbTraining: 10000 # number of training sequences
      NbTest: 1000 # number of test sequences
      time_step: 1 # subsampling step for the time series
      integration_step: ?? # integration time step for the learnt solver
      dT: 64 # number of integration time steps for the training data 
      dT_test: 32 # number of integration time steps for the test data 
      dt_forecast: ${subtract:${datamodule.input_data.param_dataset.dT},1} # number of simulation time steps, here dT-1, just using the initial condition
#      varNoise: 0. # noise set to 0
#      sampling_step: 1 # not used here (sampling step for observation data in the assimilation/forecasting case-study)
#      flagTypeMissData: -1
      
model:
  _target_: src.models_l63.Lit4dVarNet_L63_OdeSolver #src.models_l63.Lit4dVarNet_L63
  params:
    k_n_grad: ?? # number of 4dvarnet blocks of n_grad grdient-based iterations
    n_grad: 5 # number of gradient-based iterations in each block
    lr: 1e-3  # learning rate (initial value, cosine annealing learning rate)
    batch_size: 128 # batch size
    DimAE: 10 # ?? # parameter for prior Phi, parameterization of the number of channels
#    phi_param: 'unet'
#    solver: ?? #'4dvarnet-with-rnd' #?? #''4dvarnet-with-state-and-rnd' # #'4dvarnet-with-subgradients' #'4dvarnet-with-rnd-grad' #''4dvarnet-with-state-and-rnd' #
    dim_grad_solver: ?? #10 #100 #25 # dimenbsion of the hidden states of the LSTM cell
    dropout: 0. # dropout value in the LSTM solver
    alpha_prior: 0. #1e2 # weighing factor of term ||x-phi(x)||**2 in the training loss
    alpha_mse: 1.e2 # weighing factor of term ||x_true-x_pred||**2 in the training loss
    alpha_mse_implicit: 0. #1e2 # weighing factor of term ||x_true-x_pred||**2 in the training loss
    alpha_gmse: 0. #50. # weighing factor of term (time derivative) ||D_t x_true-D_t x_pred||**2 in the training loss
#    alpha_var_cost_grad: 0.0 # not used
    lr_grad: 0. #1e-2 #1.e3 # weighing factor of the firxed-step gradient descent term in 4dvarnet optimisation, \beta in equation (10)
#    lr_rnd: 0.e-3 # weighing factor of an additional Gaussian noise in (10)
#    sig_rnd_init: 0.e-2 # additive random noise in the initialization
#    sig_lstm_init: 0.e-2 # std of the random initialization of the lstm cell of the solver
#    sig_obs_noise: 0. #1e-1 # additive random noise in the observation term ||x-y||**2
    init_state: ?? #'zeros' #'last_state' #'ode_solver' # selected initialization for the simulation window
    suffix_exp: ?? #'-lstm+grad' #
#    UsePeriodicBoundary: False # not used
#    degradation_operator: 'no-degradation'  #'gaussian_smoothing' #??  # not used
#    sig_perturbation_grad: 1e-2 # not used
#    alpha_perturbation_grad: 0.9 # not used
#    gamma_degradation: 1e-1 # not used
#    type_step_lstm: 'linear-relu' #'constant'  #'linear'# # parameterization of weight decay for the lstm terme in (10)
#    param_lstm_step: 10 #20 #-1 #  # parameterization of weight decay for the lstm terme in (10)
#    dt_mse_test: 0  # not used
#    dt_mse: 0 # not used
#    post_projection: False #True #  ?? # apply a projection phi(x) after the last iteration of the solver
#    post_median_filter: False # True #  # apply a median filtering after the last iteration of the solver
#    median_filter_width: 3 # width of the meadian filter
#    keep_obs: True # force the solver to keep the state as the observed value for available observations
#    shapeData: # tensor shape for the state
#      - 3
#      - ${datamodule.input_data.param_dataset.dT} #${div:${datamodule.input_data.param_dataset.dT},${datamodule.input_data.param_dataset.integration_step}}
#      - 1
    shapeData_modGrad: #${model.params.shapeData} # shape of the gradient tensor (similar as shapeData for convLSTM solver, one-dimensional for a dense LSTM solver)
      - ${mul:${datamodule.input_data.param_dataset.dT},3} 
#    dt_forecast: ${datamodule.input_data.param_dataset.dt_forecast} 
#    dT_test: ${datamodule.input_data.param_dataset.dT_test}
#    integration_step: ${datamodule.input_data.param_dataset.integration_step}
#    time_step_ode: ${mul:${datamodule.input_data.param_dataset.time_step},${datamodule.input_data.param_dataset.integration_step}}
    base_ode_solver: 'euler' # 'rk4' # # ode solver used as baseline (reference and possible initialization)
#    use_rk4_gpu_as_target: False # True # used the rk4 GPU solver as targets
    simu_test_all_steps: False # During the training step, run the simulation for dT (False) or dT_test time steps
  Phi: # selected parameterization for operator phi
    _target_: src.models_l63.Phi_ode # src.models_l63.Phi_unet_like_bilin #    src.models_l63.Phi_unet_like_bilin # ssrc.models_l63.Phi_unet_1_layer_bis #src.models_l63.UNet_3_layers #
    #shapeData: ${model.params.shapeData}
    #DimAE: ${model.params.DimAE}
    name: 'unet-bilin' # 'odenet' # 
#    rateDropout: 0.2
  mod_Grad: # selected parameterization for the gradient-based solver
    _target_: src.solver_l63.model_Grad_with_lstm
    ShapeData: ${model.params.shapeData_modGrad}
    DimLSTM: ${model.params.dim_grad_solver} 
    rateDropout: ${model.params.dropout} 
    sig_lstm_init: ${model.params.sig_lstm_init}
    name: 'lstm-grad'

trainer:
  _target_: pytorch_lightning.Trainer
  inference_mode: False
  #gradient_clip_val: 0.5
  accelerator: gpu
  devices: 1
  precision: 32
  logger: 
    _target_: pytorch_lightning.loggers.CSVLogger
    save_dir: ${hydra:runtime.output_dir}
    name: ${hydra:runtime.choices.xp}
    version: ''
  max_epochs: 400
  callbacks:
    - _target_: src.versioning_cb.VersioningCallback
    - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val_mse
      save_top_k: 3
      filename: 
        _target_: src.models_l63.create_filename_ckpt_odesolver #create_filename_ckpt
        suffix: '-{val_loss:.4f}-{epoch:03d}'#'{val_mse:.4f}-{epoch:03d}'
        params_data: ${datamodule.input_data.param_dataset}
        params_model: ${model.params}
        name_solver: ${model.mod_Grad.name}
        name_phi: ${model.Phi.name}
        
entrypoints:
  - _target_: pytorch_lightning.seed_everything
    seed: 333
  - _target_: src.train.base_training  # src.train.fine_tuning # 
    trainer: ${trainer}
    lit_mod: ${model}
    dm: ${datamodule}
#    update_params: False
#    ckpt: 'outputs/2023-06-08/23-38-13/base_l63/checkpoints/model-l63-lstmonly+gobs+grad-4dvarnet-with-rnd-JamesData-Obs20-Noise02-unet-igrad10_04-dgrad100-val_loss=15.7092-epoch=014.ckpt'
#    ckpt: 'outputs/2023-06-08/23-23-08/base_l63/checkpoints/model-l63-lstmonly+gobs-4dvarnet-with-rnd-JamesData-Obs20-Noise02-unet-igrad10_04-dgrad100-val_loss=9.7803-epoch=092.ckpt'
#    ckpt: 'outputs/2023-06-08/18-32-56/base_l63/checkpoints/model-l63-lstmonly+gobs-4dvarnet-with-rnd-JamesData-Obs20-Noise02-unet-igrad10_08-dgrad100-val_loss=8.8370-epoch=222.ckpt'
#    ckpt: 'outputs/2023-05-24/16-55-55/base_l63/checkpoints/model-l63-4dvarnet-with-state-and-rnd-JamesData-Obs08-Noise02-unet-igrad10_04-dgrad100-val_loss=3.2830-epoch=179.ckpt'
#    ckpt: 'outputs/2023-05-23/22-14-34/base_l63/checkpoints/model-l63-4dvarnet-with-state-and-rnd-JamesData-Obs08-Noise02-unet-igrad10_04-dgrad100-val_loss=3.0530-epoch=337.ckpt'
#    ckpt: 'outputs/2023-06-10/23-30-37/base_l63/checkpoints/model-l63-lstmonly+gobs+grad-4dvarnet-with-state-and-rnd-JamesData-Obs20-Noise02-unet-igrad10_08-dgrad100-val_loss=8.5268-epoch=216.ckpt'
#  - _target_: src.test.base_testing_ode_solver #src.test.base_testing_forecast #
#   trainer: ${trainer}
#    lit_mod: ${model}
#    dm: ${datamodule}
#    num_members: 0
#    ckpt: 'outputs/2023-07-24/15-05-35/base_l63_odesolver/checkpoints/model-odesolver-l63-new-dT04_01_03_02-JamesData-odenet_10-fclstm-grad05_01_20-val_loss=0.1047-epoch=344.ckpt'
#    ckpt: 'outputs/2023-07-21/17-10-09/base_l63_odesolver/checkpoints/model-odesolver-l63-new-dT04_01_03_02-JamesData-unet-bilin_10-fclstm-grad05_02_20-val_loss=0.1530-epoch=380.ckpt'
#    ckpt: 'outputs/2023-07-19/23-56-13/base_l63_forecast/checkpoints/model-l63-l63data-forecast-dt3-dense-dT04-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad10-val_loss=0.1779-epoch=382.ckpt'
#    ckpt: 'outputs/2023-07-19/11-43-27/base_l63_forecast/checkpoints/model-l63-l63data-forecast-unet-dt3-dT04-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad10-val_loss=0.1043-epoch=377.ckpt'
#    ckpt: 'outputs/2023-07-18/23-06-57/base_l63_forecast/checkpoints/model-l63-l63-step2-forecast-dt3-dT04-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad10-val_loss=0.2576-epoch=376.ckpt'
#    ckpt: 'outputs/2023-07-18/08-30-51/base_l63_forecast/checkpoints/model-l63-l63-forecast01-ode-dT04-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad100-val_loss=40.1054-epoch=398.ckpt'
#    ckpt: 'outputs/2023-07-18/00-28-36/base_l63_forecast/checkpoints/model-l63-forecast-lstm-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_08-dgrad100-val_loss=0.0464-epoch=258.ckpt'
#    ckpt: 'outputs/2023-07-18/00-08-31//base_l63_forecast/checkpoints/model-l63-new-l63data-forecast-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad100-val_loss=0.0190-epoch=019.ckpt'
#    ckpt: 'outputs/2023-07-17/23-34-12/base_l63_forecast/checkpoints/model-l63-new-l63data-forecast-4dvarnet-with-rnd-JamesData-Obs01-Noise00-igrad10_04-dgrad100-val_loss=0.0036-epoch=043.ckpt'
#    test_fn:
#      _target_: src.utils.diagnostics
#      _partial_: true
#      test_domain:
#        time: {_target_: builtins.slice, _args_: ["2012-10-22", "2012-12-02"]}
#        lat: ${domain.test.lat}
#        lon: ${domain.test.lat}

defaults:
  - /xp/base_l63_odesolver
  